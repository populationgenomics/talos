"""
This workflow is designed to generate a minimally annotated dataset for use in Talos
this workflow is designed to be something which could be executed easily off-site by non-CPG users

* Step 1: Read a MatrixTable generated by AnnotateDataset
    - Strip out all the prior annotation content
    - Apply a region-filtered based on a provided BED file (only genic regions currently relevant to Talos)
    - Write this minimised MatrixTable to temporary storage
    - Write a sites-only VCF representation in fragments (WAY faster than a serial export)
    - Data is written into tmp

* Step 2: Reassemble the sites-only VCF fragments into a single VCF
    - This uses the gcloud compose operation, which is a rolling merge of the fragments
    - Code re-used from the rd_combiner workflow
    - Data is written into permanent storage, it's only small

* Step 3: Annotate the sites-only VCF with gnomad frequencies
    - This uses Echtvar (https://github.com/populationgenomics/images/tree/main/images/echtvar)
    - Applies a pre-extracted subset of echtvar annotations to the VCF
    - Data is written into tmp

* Step 4: Annotate the gnomAD-annotated VCF with per-transcript consequences
    - Uses bcftools csq, with a GFF3 file and reference genome
    - Data is written into tmp

* Step 5: Process the annotated VCF into a HailTable
    - Loads up the annotated VCF into a Hail Table
    - Splits up the chunky BCSQ annotations into a Hail Array of Hail Structs
    - Adds in per-transcript annotation from MANE and AlphaMissense
    - Saves the final object as a HailTable
    - Data is written into tmp

* Step 6: Load the HailTable into the final MatrixTable
    - Reads the minimised MatrixTable and the HailTable of annotations
"""

from functools import cache

import loguru
from cpg_flow import stage, targets, utils
from cpg_utils import Path

from talos.cpg_internal_scripts import cpg_flow_utils
from talos.cpg_internal_scripts.cpgflow_jobs import (
    AnnotateConsequenceUsingBcftools,
    AnnotateGnomadFrequencies,
    ComposeVcfFragments,
    ExtractVcfFromMt,
    JumpAnnotationsFromHtToFinalMt,
    SitesOnlyVcfIntoHt,
)

SHARD_MANIFEST = 'shard-manifest.txt'


@cache
def does_final_file_path_exist(cohort: targets.Cohort) -> bool:
    """
    This workflow includes the generation of a lot of temporary files and folders
    If I run it again, I don't want to accidentally regenerate all the intermediates, even though the final output
    exists. In that scenario I just want no jobs to be planned.

    This method builds the path to the final object, and checks if it exists in GCP
    If it does, we can skip all other stages
    """
    return utils.exists(
        cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            stage_name='TransferAnnotationsToMt',
            hash_value=cohort.id,
        )
        / f'{cohort.id}.mt',
    )


@stage.stage
class ExtractVcfFromDatasetMt(stage.CohortStage):
    """
    Extract some plain calls from a joint-callset.
    these calls are a region-filtered subset, limited to genic regions
    """

    def expected_outputs(self, cohort: targets.Cohort) -> dict[str, Path | str]:
        temp_prefix = cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            category='tmp',
            stage_name=self.name,
            hash_value=cohort.id,
        )
        return {
            # write path for the full (region-limited) MatrixTable, stripped of info fields
            'mt': temp_prefix / f'{cohort.id}.mt',
            # this will be the write path for fragments of sites-only VCF
            'sites_only_vcf_dir': str(temp_prefix / f'{cohort.id}_separate.vcf.bgz'),
            # this will be the file which contains the name of all fragments
            'sites_only_vcf_manifest': temp_prefix / f'{cohort.id}_separate.vcf.bgz' / SHARD_MANIFEST,
        }

    def queue_jobs(self, cohort: targets.Cohort, inputs: stage.StageInput) -> stage.StageOutput:
        outputs = self.expected_outputs(cohort)

        if does_final_file_path_exist(cohort):
            loguru.logger.info(f'Skipping {self.name} for {cohort.id}, final workflow output already exists')
            return self.make_outputs(cohort, outputs, jobs=None)

        job = ExtractVcfFromMt.make_vcf_extraction_job(
            cohort=cohort,
            output_mt=outputs['mt'],
            output_sitesonly=outputs['sites_only_vcf_dir'],
            job_attrs=self.get_job_attrs(cohort),
        )

        return self.make_outputs(cohort, outputs, jobs=job)


@stage.stage(required_stages=ExtractVcfFromDatasetMt)
class ConcatenateSitesOnlyVcfFragments(stage.CohortStage):
    def expected_outputs(self, cohort: targets.Cohort) -> Path:
        temp_prefix = cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            category='tmp',
            stage_name=self.name,
            hash_value=cohort.id,
        )
        return temp_prefix / f'{cohort.id}_sites_only_reassembled.vcf.bgz'

    def queue_jobs(self, cohort: targets.Cohort, inputs: stage.StageInput) -> stage.StageOutput:
        """Trigger a rolling merge using gcloud compose, gluing all the individual files together."""

        output = self.expected_outputs(cohort)

        extraction_outputs = inputs.as_dict(cohort, ExtractVcfFromDatasetMt)

        jobs = ComposeVcfFragments.make_condense_jobs(
            cohort_id=cohort.id,
            manifest_file=extraction_outputs['sites_only_vcf_manifest'],
            manifest_dir=extraction_outputs['sites_only_vcf_dir'],
            output=output,
            tmp_dir=cpg_flow_utils.generate_dataset_prefix(
                dataset=cohort.dataset.name,
                category='tmp',
                stage_name=self.name,
                hash_value=cohort.id,
            ),
            job_attrs=self.get_job_attrs(cohort),
        )
        return self.make_outputs(cohort, data=output, jobs=jobs)


@stage.stage(required_stages=ConcatenateSitesOnlyVcfFragments)
class AnnotateGnomadUsingEchtvar(stage.CohortStage):
    """Annotate this cohort joint-call VCF with gnomad frequencies, write to tmp storage."""

    def expected_outputs(self, cohort: targets.Cohort) -> Path:
        temp_prefix = cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            category='tmp',
            stage_name=self.name,
            hash_value=cohort.id,
        )
        return temp_prefix / f'{cohort.id}_gnomad_frequency_annotated.vcf.bgz'

    def queue_jobs(self, cohort: targets.Cohort, inputs: stage.StageInput) -> stage.StageOutput:
        output = self.expected_outputs(cohort)

        site_only_vcf = inputs.as_str(cohort, ConcatenateSitesOnlyVcfFragments)

        if does_final_file_path_exist(cohort):
            loguru.logger.info(f'Skipping {self.name} for {cohort.id}, final workflow output already exists')
            return self.make_outputs(cohort, output, jobs=[])

        job = AnnotateGnomadFrequencies.make_echtvar_job(
            cohort_id=cohort.id,
            sites_only_vcf=site_only_vcf,
            output=output,
            job_attrs=self.get_job_attrs(cohort),
        )

        return self.make_outputs(cohort, data=output, jobs=job)


@stage.stage(required_stages=AnnotateGnomadUsingEchtvar)
class AnnotateWithBcftoolsCsq(stage.CohortStage):
    """Take the VCF with gnomad frequencies, and annotate with consequences using BCFtools."""

    def expected_outputs(self, cohort: targets.Cohort) -> Path:
        temp_prefix = cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            category='tmp',
            stage_name=self.name,
            hash_value=cohort.id,
        )
        return temp_prefix / f'{cohort.id}_consequence_annotated.vcf.bgz'

    def queue_jobs(self, cohort: targets.Cohort, inputs: stage.StageInput) -> stage.StageOutput:
        output = self.expected_outputs(cohort)

        if does_final_file_path_exist(cohort):
            loguru.logger.info(f'Skipping {self.name} for {cohort.id}, final workflow output already exists')
            return self.make_outputs(cohort, output, jobs=[])

        gnomad_annotated_vcf = inputs.as_path(cohort, AnnotateGnomadUsingEchtvar)

        job = AnnotateConsequenceUsingBcftools.make_bcftools_anno_job(
            cohort_id=cohort.id,
            gnomad_vcf=gnomad_annotated_vcf,
            output=output,
            job_attrs=self.get_job_attrs(cohort),
        )

        return self.make_outputs(cohort, data=output, jobs=job)


@stage.stage(required_stages=AnnotateWithBcftoolsCsq)
class AnnotatedVcfIntoHt(stage.CohortStage):
    """Join the annotated sites-only VCF with AlphaMissense, and with gene/transcript information."""

    def expected_outputs(self, cohort: targets.Cohort) -> Path:
        temp_prefix = cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            category='tmp',
            stage_name=self.name,
            hash_value=cohort.id,
        )
        return temp_prefix / f'{cohort.id}_annotations.ht'

    def queue_jobs(self, cohort: targets.Cohort, inputs: stage.StageInput) -> stage.StageOutput:
        output = self.expected_outputs(cohort)

        if does_final_file_path_exist(cohort):
            loguru.logger.info(f'Skipping {self.name} for {cohort.id}, final workflow output already exists')
            return self.make_outputs(cohort, output, jobs=[])

        bcftools_vcf = inputs.as_str(cohort, AnnotateWithBcftoolsCsq)

        job = SitesOnlyVcfIntoHt.make_vcf_to_ht_job(
            cohort_id=cohort.id,
            bcftools_vcf=bcftools_vcf,
            output_ht=output,
            tmp_dir=cpg_flow_utils.generate_dataset_prefix(
                dataset=cohort.dataset.name,
                category='tmp',
                stage_name=self.name,
                hash_value=cohort.id,
            )
            / f'{cohort.id}_annotation_checkpoint',
            job_attrs=self.get_job_attrs(cohort),
        )

        return self.make_outputs(cohort, data=output, jobs=job)


@stage.stage(
    required_stages=[AnnotatedVcfIntoHt, ExtractVcfFromDatasetMt],
    analysis_type='talos_prep',
)
class TransferAnnotationsToMt(stage.CohortStage):
    """Take the variant MatrixTable and a HT of annotations, combine into a final MT."""

    def expected_outputs(self, cohort: targets.Cohort) -> Path:
        temp_prefix = cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            stage_name=self.name,
            hash_value=cohort.id,
        )
        return temp_prefix / f'{cohort.id}.mt'

    def queue_jobs(self, cohort: targets.Cohort, inputs: stage.StageInput) -> stage.StageOutput:
        output = self.expected_outputs(cohort)

        if does_final_file_path_exist(cohort):
            loguru.logger.info(f'Skipping {self.name} for {cohort.id}, final workflow output already exists')
            return self.make_outputs(cohort, output, jobs=[])

        # get the region-limited MT
        mt = inputs.as_str(cohort, ExtractVcfFromDatasetMt, 'mt')

        # get the table of compressed annotations
        annotations = inputs.as_str(cohort, AnnotatedVcfIntoHt)

        job = JumpAnnotationsFromHtToFinalMt.make_annotation_transfer_job(
            cohort_id=cohort.id,
            annotations_ht=annotations,
            input_mt=mt,
            output_mt=output,
            job_attrs=self.get_job_attrs(cohort),
        )

        return self.make_outputs(cohort, data=output, jobs=job)
