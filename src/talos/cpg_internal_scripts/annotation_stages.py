"""
Rewrite of the annotation workflow

This workflow is designed to generate a minimally annotated dataset for use in Talos
this workflow is designed to be something which could be executed easily off-site by non-CPG users

* Step 1: Read a MatrixTable generated by AnnotateDataset
    - Strip out all the prior annotation content
    - Apply a region-filtered based on a provided BED file (only genic regions currently relevant to Talos)
    - Write this minimised MatrixTable to temporary storage
    - Write a sites-only VCF representation in fragments (WAY faster than a serial export)
    - Data is written into tmp

* Step 2: Reassemble the sites-only VCF fragments into a single VCF
    - This uses the gcloud compose operation, which is a rolling merge of the fragments
    - Code re-used from the rd_combiner workflow
    - Data is written into permanent storage, it's only small

* Step 3: Annotate the sites-only VCF with gnomad frequencies
    - This uses Echtvar (https://github.com/populationgenomics/images/tree/main/images/echtvar)
    - Applies a pre-extracted subset of echtvar annotations to the VCF
    - Data is written into tmp

* Step 4: Annotate the gnomAD-annotated VCF with per-transcript consequences
    - Uses bcftools csq, with a GFF3 file and reference genome
    - Data is written into tmp

* Step 5: Process the annotated VCF into a HailTable
    - Loads up the annotated VCF into a Hail Table
    - Splits up the chunky BCSQ annotations into a Hail Array of Hail Structs
    - Adds in per-transcript annotation from MANE and AlphaMissense
    - Saves the final object as a HailTable
    - Data is written into tmp

* Step 6: Load the HailTable into the final MatrixTable
    - Reads the minimised MatrixTable and the HailTable of annotations

And somewhat in parallel to that:

* Step M1: If availalable, pick up a Mitochondrial joint-vcf
    - this will be filed in Metamist under the dataset, called "merged_mito.vcf.bgz"
    - Mito variant calling is based on "chrM" (Hail likes "chrM")
    - Run BCFtools consequence annotation on that VCF
    - Run the step.5 from above (reformatting annotations)
    - We don't have Echtvar annotations for chrM(?)

* Step M2: Concat the reformatted Mito MatrixTable to the whole-genome
"""

from functools import cache

import loguru
from sortedcontainers import SortedDict

from cpg_flow import stage, targets, utils
from cpg_flow.stage import StageInput, StageOutput
from cpg_flow.targets import MultiCohort
from cpg_utils import Path, config, to_path

from talos.cpg_internal_scripts import cpg_flow_utils
from talos.cpg_internal_scripts.cpgflow_jobs import (
    annotate_csq_with_bcftools,
    annotate_splice_ai,
    annotate_with_echtvar,
    extract_vcfs_from_mt,
    generate_alphamissense_zip,
    vcf_into_matrixtable,
)

SHARD_MANIFEST = 'shard-manifest.txt'

# use to record the number of fragments in each shard manifest
FRAGMENTS: dict[str, int] = {}


@cache
def does_final_file_path_exist(cohort: targets.Cohort) -> bool:
    """
    This workflow includes the generation of a lot of temporary files and folders
    If I run it again, I don't want to accidentally regenerate all the intermediates, even though the final output
    exists. In that scenario I just want no jobs to be planned.

    This method builds the path to the final object, and checks if it exists in GCP
    If it does, we can skip all other stages
    """

    # if this is being forced, state that the final output can't be reused
    if config.config_retrieve(['workflow', 'force_rerun'], False):
        return False

    return utils.exists(
        cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            stage_name='AnnotateSpliceAi',
            hash_value=cohort.id,
        )
        / f'{cohort.id}.mt',
    )


@cache
def get_manifest_fragments(vcf_dir: str) -> SortedDict:
    """"""
    dir_path = to_path(vcf_dir)
    manifest = dir_path / SHARD_MANIFEST

    if not manifest.exists():
        raise FileNotFoundError(
            f"At lease one cohort needs, but doesn't have, VCF fragments: {manifest}. "
            f"Re-run the workflow with last_stages='ExtractVcfFromDatasetMt'",
        )

    with manifest.open() as f:
        return SortedDict({f'part_{i}': line.rstrip() for i, line in enumerate(f)})


@stage.stage
class EncodeAlphamissense(stage.MultiCohortStage):
    """ """

    def expected_outputs(self, _multicohort: MultiCohort) -> dict[str, Path]:
        return {
            'zip': to_path(config.config_retrieve(['storage', 'common', 'default'])) / 'alphamissense_echtvar.zip',
        }

    def queue_jobs(self, multicohort: MultiCohort, inputs: StageInput) -> StageOutput:
        """Do the thing."""
        outputs = self.expected_outputs(multicohort)
        job = generate_alphamissense_zip.encode_alphamissense(outputs['zip'])
        return self.make_outputs(multicohort, outputs, jobs=job)


@stage.stage
class ExtractVcfFromDatasetMt(stage.CohortStage):
    """
    Extract some plain calls from a joint-callset.
    these calls are a region-filtered subset, limited to genic regions
    """

    def expected_outputs(self, cohort: targets.Cohort) -> dict[str, Path | str]:
        temp_prefix = cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            category='tmp',
            stage_name=self.name,
            hash_value=cohort.id,
        )
        return {
            'vcf_dir': str(temp_prefix / f'{cohort.id}_separate.vcf.bgz'),
            'vcf_manifest': temp_prefix / f'{cohort.id}_separate.vcf.bgz' / SHARD_MANIFEST,
        }

    def queue_jobs(self, cohort: targets.Cohort, inputs: stage.StageInput) -> stage.StageOutput:
        outputs = self.expected_outputs(cohort)

        if does_final_file_path_exist(cohort):
            loguru.logger.info(f'Skipping {self.name} for {cohort.id}, final workflow output already exists')
            return self.make_outputs(cohort, outputs, jobs=None)

        job = extract_vcfs_from_mt.make_vcf_extraction_job(
            cohort=cohort,
            output=outputs['vcf_dir'],
            job_attrs=self.get_job_attrs(cohort),
        )

        return self.make_outputs(cohort, outputs, jobs=job)


# Ok, there's a change in behaviour here - these stages are carried out on parallel VCF shards


@stage.stage(required_stages=[EncodeAlphamissense, ExtractVcfFromDatasetMt])
class AnnotateUsingEchtvar(stage.CohortStage):
    """Annotate VCF with gnomad frequencies and AlphaMissense, write to tmp storage."""

    def expected_outputs(self, cohort: targets.Cohort) -> dict[str, Path | str]:
        temp_prefix = cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            category='tmp',
            stage_name=self.name,
            hash_value=cohort.id,
        )
        return {
            'success': temp_prefix / 'success.txt',
            'fragment_template': str(temp_prefix / '{part}.vcf.bgz'),
        }

    def queue_jobs(self, cohort: targets.Cohort, inputs: stage.StageInput) -> stage.StageOutput:
        outputs = self.expected_outputs(cohort)

        if does_final_file_path_exist(cohort):
            loguru.logger.info(f'Skipping {self.name} for {cohort.id}, final workflow output already exists')
            return self.make_outputs(cohort, outputs, jobs=[])

        manifest = inputs.as_str(cohort, ExtractVcfFromDatasetMt, key='vcf_dir')
        fragments = get_manifest_fragments(manifest)
        am_zip = inputs.as_str(target=cohort, stage=EncodeAlphamissense, key='zip')

        jobs = annotate_with_echtvar.make_echtvar_job(
            cohort_id=cohort.id,
            fragments=fragments,
            am_zip=am_zip,
            outputs=outputs,
            job_attrs=self.get_job_attrs(cohort),
        )

        return self.make_outputs(cohort, data=outputs, jobs=jobs)


@stage.stage(required_stages=[ExtractVcfFromDatasetMt, AnnotateUsingEchtvar])
class AnnotateWithBcftoolsCsq(stage.CohortStage):
    """Take the VCFs with gnomad/alphamissense annotations, add consequences using BCFtools."""

    def expected_outputs(self, cohort: targets.Cohort) -> dict[str, Path | str]:
        temp_prefix = cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            category='tmp',
            stage_name=self.name,
            hash_value=cohort.id,
        )
        return {
            'success': temp_prefix / 'success.txt',
            'fragment_template': str(temp_prefix / '{part}.vcf.bgz'),
        }

    def queue_jobs(self, cohort: targets.Cohort, inputs: stage.StageInput) -> stage.StageOutput:
        outputs = self.expected_outputs(cohort)

        if does_final_file_path_exist(cohort):
            loguru.logger.info(f'Skipping {self.name} for {cohort.id}, final workflow output already exists')
            return self.make_outputs(cohort, outputs, jobs=[])

        manifest = inputs.as_str(cohort, ExtractVcfFromDatasetMt, key='vcf_dir')
        fragments = get_manifest_fragments(manifest)
        echtvar_template = inputs.as_str(target=cohort, stage=AnnotateUsingEchtvar, key='fragment_template')

        jobs = annotate_csq_with_bcftools.make_bcftools_anno_jobs(
            cohort_id=cohort.id,
            fragments=list(fragments.keys()),
            echtvar_template=echtvar_template,
            outputs=outputs,
            job_attrs=self.get_job_attrs(cohort),
        )

        return self.make_outputs(cohort, data=outputs, jobs=jobs)


# changes from here - parallel stages to create a range of MTs, then gather them in SpliceAi
@stage.stage(required_stages=[ExtractVcfFromDatasetMt, AnnotateWithBcftoolsCsq])
class AnnotatedVcfIntoMt(stage.CohortStage):
    def expected_outputs(self, cohort: targets.Cohort) -> dict[str, Path | str]:
        temp_prefix = cpg_flow_utils.generate_dataset_prefix(
            dataset=cohort.dataset.name,
            category='tmp',
            stage_name=self.name,
            hash_value=cohort.id,
        )
        return {
            'success': temp_prefix / 'success.txt',
            'fragment_template': str(temp_prefix / '{part}.mt'),
        }

    def queue_jobs(self, cohort, inputs: stage.StageInput) -> stage.StageOutput:
        outputs = self.expected_outputs(cohort)

        if does_final_file_path_exist(cohort):
            loguru.logger.info(f'Skipping {self.name} for {cohort.id}, final workflow output already exists')
            return self.make_outputs(cohort, outputs, jobs=[])

        manifest = inputs.as_str(cohort, ExtractVcfFromDatasetMt, key='vcf_dir')
        fragments = get_manifest_fragments(manifest)
        bcftools_template = inputs.as_str(target=cohort, stage=AnnotateWithBcftoolsCsq, key='fragment_template')

        jobs = vcf_into_matrixtable.create_mt_ingest_jobs(
            cohort_id=cohort.id,
            fragments=list(fragments.keys()),
            bcftools_template=bcftools_template,
            checkpoint=cpg_flow_utils.generate_dataset_prefix(
                dataset=cohort.dataset.name,
                category='tmp',
                stage_name=self.name,
                hash_value=cohort.id,
            )
            / f'{cohort.id}_{{part}}annotation_checkpoint',
            outputs=outputs,
            job_attrs=self.get_job_attrs(cohort),
        )

        return self.make_outputs(cohort, data=outputs, jobs=jobs)


@stage.stage(
    required_stages=[ExtractVcfFromDatasetMt, AnnotatedVcfIntoMt],
    analysis_type='talos_prep',
)
class AnnotateSpliceAi(stage.CohortStage):
    """Take the annotated MatrixTable and add SpliceAi annotations. Private CPG stage."""

    def expected_outputs(self, cohort: targets.Cohort) -> Path:
        return (
            cpg_flow_utils.generate_dataset_prefix(
                dataset=cohort.dataset.name,
                stage_name=self.name,
                hash_value=cohort.id,
            )
            / f'{cohort.id}.mt'
        )

    def queue_jobs(self, cohort: targets.Cohort, inputs: stage.StageInput) -> stage.StageOutput:
        output = self.expected_outputs(cohort)

        manifest = inputs.as_str(cohort, ExtractVcfFromDatasetMt, key='vcf_dir')
        fragments = get_manifest_fragments(manifest)
        mt_template = inputs.as_str(target=cohort, stage=AnnotatedVcfIntoMt, key='fragment_template')

        job = annotate_splice_ai.add_job(
            cohort_id=cohort.id,
            fragments=list(fragments.keys()),
            mt_template=mt_template,
            output_mt=str(output),
            job_attrs=self.get_job_attrs(cohort),
        )

        return self.make_outputs(cohort, data=output, jobs=job)
